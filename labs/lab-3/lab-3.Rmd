---
title: "Lab 3: Univariate Regression (II) & GLM"
output: 
  html_document: 
    fig_caption: yes
    theme: cosmo
    toc: yes
    toc_depth: 3
    toc_float: TRUE
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, rows.print = 10)

# suppress scientific notation
options(scipen = 999)
```


# Purpose

Today we will briefly review of univariate regression and then will discuss how to summarize and visualize uncertainty about regression models using a variety of plotting methods. We will then touch on how to estimate regression coefficients using matrix algebra. Lastly, we will introduce the topic of the General Linear Model and demonstrate how GLM can be used to understand all of the statistical tests we have learned so far (*t*-tests, ANOVA, correlations, regressions) within one unifying framework. 

To quickly navigate to the desired section, click one of the following links:

1. [Visualizing uncertainty](#uncertainty)
1. [Regression with matrix algebra](#matrix)
1. [The General Linear Model](#glm)

You will need to load the following libraries to follow along with today's lab. If you don't have any of these packages installed, please do so now. 

```{r message=FALSE}
library(tidyverse) # for plotting and data wrangling
library(rio) # for importing data
library(psych) # for stats functions
library(broom) # for cleaning up output
library(sjPlot) # for plotting
library(ggpubr) # for plotting
library(carData) # for Guyer dataset
```

***

# Visualizing uncertainty{#uncertainty}

* We're going to use the same dataset from previous labs about the relationship between conscientiousness and self-rated health. 

## Data and review

* Load in the data

```{r}
health <- import("https://raw.githubusercontent.com/uopsych/psy612/master/labs/lab-3/data/consc_health.csv")
```
<br>

* Recall how we wrote out our model

$$Y_i = b_0 + b_1X_i + e_i$$

$$health_i = b_0 + b_1consc_i + e_i$$
<br>

* Here's how we specified the model in R

```{r}
model <- lm(sr_health ~ consc, data = health)
```
<br>

* Here are our cofficients...

```{r echo=FALSE}
tidy(model) %>% #
  rename(coefficient = term,
        b = estimate,
        SE = std.error,
        t = statistic,
        p = p.value) %>%
  mutate(p = ifelse(p > .001, round(p, 3), "< .001")) %>% 
  knitr::kable(digits = c(NA, 2, 2, 2, 3), 
               caption = "Results of Regressing Self-Reported Health on Conscientiousness") 
```
<br>

> **Question:** What do the intercept and slope mean? What do the *t*-values tell us? 

> **Answer:** Intercept = the expected value for self-rated health when conscientiousness is 0. Slope = the magnitude of the relationship between conscientiousness and self-rated health: for every 1-unit increase in conscientiousness, we expect a 0.49-unit increase in self-rated health. *t*-values are from a one-sample t-test assessing whether the slope and intercept are significantly different from 0; the *t* values represent the ratio of signal to noise (i.e. each b divided by its standard error).

## Confidence intervals

* Our `b's` (intercept and slope) are *estimates* from our sample of true population parameters ($\beta$'s). Remember that whenever we calculate an estimate of something, we should also determine how precise our estimate is. This is where standard errors and confidence intervals come in. 

* Recall the formula for calculating confidence intervals:

$$CI_b = b \pm CV(SE_b)$$

* In [Minihack 1](#minihack1) you will get some practice using this formula to calculate confidence intervals around regression coefficients. For now, we will use a much easier method: `stats::confint()`. This function takes in a fitted model object as the first argument. By default it will give you 95% CI's. 

```{r }
confint(model)
```

>**Question:** What does these 95% CI for the slope of conscientiousness mean in plain English? 

>**Answer:** If we were to repeat this experiment over and over again, sampling from the same population, 95% of the time the slope we calculate would be between 0.25 and 0.73 (i.e. in 19 out of every 20 experiments we'd get a slope in this interval).

## Confidence bands

* In addition to estimating precision around the our coefficients, we can also estimate our precision around each predicted value, $\hat{Y_i}$. These standard errors are generated by `broom::augment()` (and are labeled `.se.fit`).

```{r}
model %>% # start with our model object
  augment() %>% # from broom package; gives us fitted values, residuals, etc.
  select(sr_health, .fitted, .se.fit) # select relevant variables
```
<br>

* If we were to string all of this information together, it would generate a confidence **band** around our regression line. As we've seen already with previous examples, it's really easy to get this confidence band when creating a scatter plot by adding `geom_smooth(method = "lm")`. 


```{r}
health %>%
  ggplot(aes(x = consc, y = sr_health)) + 
  geom_point(alpha = 0.3) + 
  geom_smooth(method = "lm") + # adds a layer that includes the regression line & 95% confidence band
  labs(x = "Conscientiousness", y = "Self-rated health") +
  theme_minimal()
```


* The animation below is an example of a ["Hypothetical Outcomes Plot"](https://github.com/wilkelab/ungeviz){target="_blank"} (HOP) that demonstrates what this 95% CI band really means. If we were to repeat this same experiment over and over and estimated our regression line each time, 95% of the time our regression line would fall within this confidence band.

<!-- ```{r echo=FALSE} -->
<!-- library(ungeviz) -->
<!-- library(gganimate) -->
<!-- library(transformr) -->
<!-- library(gifski) -->

<!-- set.seed(012220) -->

<!-- boots <- bootstrapper(100) -->

<!-- p <- health %>% -->
<!--   ggplot(aes(x = consc, y = sr_health)) + -->
<!--   geom_smooth(method = "lm", color = NA) + -->
<!--   geom_point(alpha = 0.3) + -->
<!--   geom_point(data = boots, aes(group = .row)) + -->
<!--   geom_smooth(data = boots, method = "lm", fullrange = TRUE, se = FALSE) + -->
<!--   theme_minimal() + -->
<!--   labs(x = "Conscientiousness", y = "Self-rated health") + -->
<!--   transition_states(.draw, 1, 1) + -->
<!--   enter_fade() + -->
<!--   exit_fade() + -->
<!--   ease_aes() -->

<!-- animate(p, fps = 3) -->

<!-- ``` -->
<!-- <br> -->

## Prediction bands

* A regression line, by definition, corresponds to the line that gives the *mean* value of Y corresponding to each possible value of X, i.e. E(Y|X). 

* In addition, we can also predict an individual's score ($Y_i$) given any value of x. From our regression model, we have the following equation that represents how our model characterizes the relationship between conscientiousness and self-rated health: 

$$\hat{health}_{i} = 1.6569733 + 0.4904059 * consc_{i}$$

* For example, if we know someone's conscientiousness score is `3.5`, we can easily predict their score for self-rated health according to our model: 

$$\hat{health} = 1.6569733 + 0.4904059*3.5 = 3.374$$

* The `predict()` function gives us an easy way to get the predicted `Y` values from all the `X` values in our dataset.  

```{r}
predict(model)
```

* This should look familiar, as we already have gotten this information from `broom::augment()`. 

```{r}
augment(model)$.fitted
```

* We can use this information to create prediction bands

```{r}
predicted <- predict(model, interval = "prediction")
predicted
```


```{r}
new_df <- cbind(health, predicted)
new_df
```


```{r}
new_df %>% 
  ggplot(aes(x = consc, y = sr_health)) +
  geom_point() +
  geom_smooth(method=lm,se=TRUE) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), fill = "blue", alpha = 0.1) + # prediction band
  labs(x = "Conscientiousness", y = "Self-rated health") +
  theme_minimal()
```

>**Question:** Why is the prediction band wider than the confidence band around our regression line?

>**Answer:** Because there is greater variation in predicting an individual value rather than a collection of individual values (i.e., the mean) the prediction band is wider

* But remember, we can plug *any* `X` value (i.e. conscientiousness score) into our equation and predict the corresponding `Y` value (i.e. self-rated health score). And when I say *any* `X` value, I mean that it doesn't have to be an actual observation from our sample. We can also predict out-of-sample! 

* To illustrate this, we'll first take a data frame that has new X values (conscientiousness scores). Let's pretend that we collected this data from a completely new sample of people.

```{r}
# read in new data
consc_data_new <- import("https://raw.githubusercontent.com/uopsych/psy612/master/labs/lab-3/data/consc_new.csv")

# let's view it
consc_data_new
```
<br>

* Now we can predict `Y` values for these out-of-sample `X` values based on the coefficients from our model. We just have to tell the `predict()` function what our `newdata` is. 

```{r}
predict(model, newdata = consc_data_new)
```

## Other visualization tools

* `sjPlot::plot_model()`

```{r}
plot_model(model = model,    # name of model object
           type = "pred",    # show predicted values (i.e. regression line)
           show.data = TRUE, # include data points on plot
           jitter = TRUE)    # add small amount of random variation to  to prevent overlap
```

* `ggpubr::ggscatter()`

```{r}
ggscatter(data = health,              # name of data.frame
          x = "consc",                # IV (must be quoted)
          y = "sr_health",            # DV (must be quoted)
          add = "reg.line",           # add regression line
          xlab = "Conscientiousness", # x-axis label
          ylab = "Self-rated health", # y-axis label
          conf.int = TRUE,            # show 95% confidence band around regression line
          cor.coef = TRUE)            # display correlation coefficient and p-value
```


***

# Regression with matrix algebra

* Create the **`X`** matrix 

```{r}
x_mat <- health %>% 
  mutate(ones = rep(1, nrow(.))) %>% 
  select(ones, consc) %>% 
  as.matrix()

print(x_mat)
```

* Create the **`Y`** matrix 

```{r}
y_mat <- health %>% 
  select(sr_health) %>% 
  as.matrix()

print(y_mat)
```

* Apply the matrix algebra formula to solve for the **`b`** matrix

$$\mathbf{b} = (\mathbf{X'X})^{-1} \mathbf{X'Y}$$

```{r}
solve(t(x_mat) %*% x_mat) %*% (t(x_mat) %*% y_mat)
```

* The `b`'s we just solved for using matrix algebra match the coefficients we get from `lm()`! 

```{r}
model$coefficients
```

***

# The General Linear Model

```{r}
guyer <- carData::Guyer
```

* Look at the first few rows

```{r}
head(guyer)
```

* Look at the structure of the data 

```{r}
str(guyer)
```

* Dummy code the `condition` variable

```{r}
guyer <-  guyer %>% 
  mutate(condition = case_when(condition == "public" ~ 0,
                               condition == "anonymous" ~ 1))

```


## *t*-test

```{r}
t_test <- t.test(formula = cooperation ~ condition, data = guyer, var.equal = TRUE)
t_test
```

## Correlation 

```{r}
cor_test <- cor.test(formula = ~ cooperation + condition, data = guyer) # note the one-sided formula
cor_test
```

## ANOVA

```{r}
anova_test <- aov(formula = cooperation ~ condition, data = guyer)
summary(anova_test)
```

## Regression

```{r}
regression <- lm(formula = cooperation ~ condition, data = guyer)
summary(regression)
```

***

# Minihacks

## Minihack 1: Calculating confidence intervals{#minihack1}

* For this minihack, we will refer back to the example about conscientiousness and health. We used `confint()` to calculate the 95% CI for our regression coefficients (slope and intercept). Your job is start with the 

```{r eval=FALSE}
model <- lm(sr_health ~ consc, data = health)
```


1. Calculating confidence intervals "by hand". 

```{r}
model <- lm(sr_health ~ consc, data = health)
model_summary <- summary(model)

# extract coefficients
int <- model_summary$coefficients[[1,1]]
slope <- model_summary$coefficients[[2,1]]

# extract standard errors of coefficients
int_se <- model_summary$coefficients[[1,2]]
slope_se <- model_summary$coefficients[[2,2]]

# df for t-distribution (will be denominator df from F statistic)
df <- model_summary$fstatistic[["dendf"]]

# intercept 95% CI 
int_ci_l <- int - qt(.975, df = df)*int_se
int_ci_u <- int + qt(.975, df = df)*int_se

# slope 95% CI 
slope_ci_l <- slope - qt(.975, df = df)*slope_se
slope_ci_u <- slope + qt(.975, df = df)*slope_se
```

2. Verify that your answer corresponds to the result from `confint()`. 

```{r}
confint(model)
```


***

## Minihack 2: Matrix algebra

Refer back to the [lab on matrix algebra](https://uopsych.github.io/psy611/labs/lab-3.html){target="_blank"} from 611 if needed. 


1. Create the Matrix $\mathbf{X}$ & Vector $\mathbf{Y}$

```{r}
# your code here
```

2. Produce $\mathbf{X'}$ (also called $\mathbf{X^T}$ or the transpose of $\mathbf{X}$)

```{r}
# your code here
```

3. Produce $\mathbf{X'X}^{-1}$ (inverse of $\mathbf{X'X}$)

```{r}
# your code here
```

4. Produce $\mathbf{X'Y}$

```{r}
# your code here
```

5. Produce $\mathbf{B = (X'X)}^{-1}\mathbf{X'Y}$

```{r}
# your code here
```

6. Confirm your results using the `lm()` function in R.

```{r}
# your code here
```


## 1. Creating the Matrix $\mathbf{X}$

The first thing we need to do is create the matrix $\mathbf{X}$, which is also called the *design matrix*. We are going to enter the matrix 'by hand' in R, but know that you could also enter the data/matrix into a .csv file and read that file into R.

To enter a matrix in R, we use the `matrix()` function. If you look at the help documentation (by running `?matrix`) you can see that the first argument is the `data` (which can be entered as a list or vector of values). Then we specify the dimensions of the matrix using the `nrow =` and `ncol =` arguments. Finally, we tell it if the values are in being entered row-wise or column-wise by specifying `byrow =` (which can be `TRUE` or `FALSE`).

So, now let's create our 4x3 design matrix. We will enter it column-wise.

```{r creating_design_matrix_1}
# minor note, though we usually refer to matrices
# with capital letters, we'll use a lower case
# x. R is case sensitive, and I recommend always using
# lower case letters (a style called "snake_case")
x <- matrix(c(1, 1, 1, 1, # what goes here?
              1, 2, 7, 8, # X1 column
              1, 2, 2, 1), # X2 column 
            nrow = 4, ncol = 3, 
            byrow = FALSE)
```

Next, take a look at the matrix to make sure it looks right.
```{r creating_design_matrix_2}
x
```

Yep, looks like we entered it correctly. 

Now we'll create the vector $\mathbf{Y}$. This is just like creating any vector in R, so this should look very familiar. The main thing is to make sure the order is correct (i.e., matches the design matrix).

```{r creating_Y_vector}
y <- c(4, 5, 12, 14)
```

as always, we can take a look:
```{r}
y
```

Okay, that looks right too. Time to move on to step 2.

## 2. Producing $\mathbf{X'}$

Next, we need to produce $\mathbf{X'}$ or the transpose of **X**. To do that, we can use the `t()` function in R. All it requires is a matrix or dataframe, and then it returns the transpose of that matrix or dataframe. We'll name this x_t

```{r obtaining_x_t}
x_t <-  t(x)
```

And take a look at it alongside x to make sure everything looks OK.

```{r obtaining_x_t_2}
x
x_t
```

Yep, looks like we sucessfully obtained $\mathbf{X'}$

## 3. Producing $\mathbf{X'X}^{-1}$ 

Next we need to obtain  $\mathbf{X'X}^{-1}$ or the inverse of $\mathbf{X'X}$. We'll do this in two steps. First, we'll create $\mathbf{X'X}$ by multiplying the original design matrix $\mathbf{X}$ (or `x` in R) by its transpose $\mathbf{X'}$ (or x_t in R). In R, we use `%*%` for matrix multiplication. Note that using `*` it would instead do element-wise multiplication. We'll create $\mathbf{X'X}$ and call it `xt_x`

```{r produce_xtx}
xt_x <- x_t %*% x
xt_x
```

Then, we invert this new matrix using the `solve()` function. We'll call the resulting matrix `xtx_inv`.

```{r invert_xtx}
xtx_inv <- solve(xt_x)
xtx_inv
```

## 4. Producing $\mathbf{X'Y}$

Next, we need to obtain $\mathbf{X'Y}$, which we can get by again using the matrix multiplication operator, `%*%`.
```{r prduce_xty}
xt_y <- x_t %*% y
xt_y
```

## 5. Produce $\mathbf{B}$

Next, we have everything we need to obtain the coefficient matrix $\mathbf{B}$. Remember:

$$\mathbf{B} = (\mathbf{X'X})^{-1}(\mathbf{X'Y})$$
So we just need to use matrix multiplication to get the product of $(\mathbf{X'X})^{-1}$ & $(\mathbf{X'Y})$ which we just created in steps 3 and 4 above. 

```{r produce_B}
b <- xtx_inv %*% xt_y
```

And, let's see what the values for the coefficients are:
```{r produce_B_2}
b
```

Okay, now we've completed conducting a multiple regression using matrix algebra. Now we'll complete the last step and make sure our calculations for b match the results from R's `lm()` function.

## 6. Confirming your results with `lm()`

First, we need to combine the matrices x and y into a single dataframe. To keep things straight, we'll first give the columns of x column names. We'll do this using the `colnames()` function. We'll call the columns `intercept`, `x1`, and `x2`. We can assign new column names to an obect like so:
```{r checking_our_work_1}
colnames(x) <- c("intercept", "x1", "x2")
# check that it worked
x
```
We will do the same thing for y. However, because of how R represents and names things, we need to explicitly tell R y is a matrix in order to give it column names (otherwise it will think we're trying to name each element in the vector).
```{r checking_our_work_2}
y <- as.matrix(y)
colnames(y) <- "y"
y
```

There, that looks right. Now put them together, and put them into a dataframe:

```{r}
df <- as.data.frame(cbind(y, x))
# note, if you wanted to do this
# in the tidyverse style, with pipes, it
# would look like this:
#library(tidyverse)
#df <- cbind(y, x) %>% 
#  as.data.frame()
```

Next, we conduct the regression using the `lm()` function. Now we needed the intercept column when we calculated the regression with matrix algebra, but we don't need it for the `lm()` function because `lm()` puts that in automatically (this should hopefully seem familar, since we didn't explicitly enter the intercept in `lm()` in 611).

```{r}
model <- lm(y ~ x1 + x2, # remember, outcome ~ predictors
            data = df) # enter the dataframe
```

Now we can see the cofficients
```{r}
summary(model)
```
And let's see those coefficient estimates next to the estimates we obtained with matrix algebra to check our work. We'll put them together into a little table using `cbind()`, label the columns to make it clear which column is the from the matrix calculations and which is from the `lm()` call.

Our B vector is easy. The cofficients for the lm model can be accessed with `model$coefficients`. In `lm` results objects, the model coefficients are stored in a vector called `coefficients`, so `model$coefficients` tells R to give us just that vector. We'll put that as the second object in `cbind()` 
```{r}
comparison_table <- cbind(b, # coefficient matrix
                          model$coefficients) # coefficients from lm

colnames(comparison_table) <- c("matrix", "lm")
comparison_table
```

Yep, everything lines up perfectly. Looks like we our matrix calculations were correct.


***

## Minihack 3

***
