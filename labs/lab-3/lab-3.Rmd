---
title: "Lab 3: Univariate Regression (II) & GLM"
output: 
  html_document: 
    fig_caption: yes
    theme: cosmo
    toc: yes
    toc_depth: 3
    toc_float: TRUE
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, rows.print = 10)

# suppress scientific notation
options(scipen = 999)
```


# Purpose

INTRO 

To quickly navigate to the desired section, click one of the following links:

1. [Visualizing uncertainty](#uncertainty)
1. [Regression with matrix algebra](#matrix)
1. [The General Linear Model](#glm)

You will need to load the following libraries to follow along with today's lab. If you don't have any of these packages installed, please do so now. 

```{r message=FALSE}
library(tidyverse) # for plotting and data wrangling
library(rio) # for importing data
library(psych) # for stats functions
library(broom) # for cleaning up output
library(sjPlot) # for plotting
library(ggpubr) # for plotting
```

***

# Visualizing uncertainty{#uncertainty}

* We're going to use the same dataset from previous labs about the relationship between conscientiousness and self-rated health. 

## Data and review

* Load in the data

```{r}
health <- import("https://raw.githubusercontent.com/uopsych/psy612/master/labs/lab-3/data/consc_health.csv")
```
<br>

* Recall how we wrote out our model

$$Y_i = b_0 + b_1X_i + e_i$$

$$health_i = b_0 + b_1consc_i + e_i$$
<br>

* Here's how we specified the model in R

```{r}
model <- lm(sr_health ~ consc, data = health)
```
<br>

* Here are our cofficients...

```{r echo=FALSE}
tidy(model) %>% #
  rename(coefficient = term,
        b = estimate,
        SE = std.error,
        t = statistic,
        p = p.value) %>%
  mutate(p = ifelse(p > .001, round(p, 3), "< .001")) %>% 
  knitr::kable(digits = c(NA, 2, 2, 2, 3), 
               caption = "Results of Regressing Self-Reported Health on Conscientiousness") 
```
<br>

> **Question:** What do the intercept and slope mean? What do the *t*-values tell us? 

> **Answer:** Intercept = the expected value for self-rated health when conscientiousness is 0. Slope = the magnitude of the relationship between conscientiousness and self-rated health: for every 1-unit increase in conscientiousness, we expect a 0.49-unit increase in self-rated health. *t*-values are from a one-sample t-test assessing whether the slope and intercept are significantly different from 0; the *t* values represent the ratio of signal to noise (i.e. each b divided by its standard error).

## Confidence intervals

* Our `b's` (intercept and slope) are *estimates* from our sample of true population parameters ($\beta$'s). Remember that whenever we calculate an estimate of something, we should also determine how precise our estimate is. This is where standard errors and confidence intervals come in. 

* Recall the formula for calculating confidence intervals:

$$CI_b = b \pm CV(SE_b)$$

* In [Minihack 1](#minihack1) you will get some practice using this formula to calculate confidence intervals around regression coefficients. For now, we will use a much easier method: `stats::confint()`. This function takes in a fitted model object as the first argument. By default it will give you 95% CI's. 

```{r }
confint(model)
```

>**Question:** What does these 95% CI for the slope of conscientiousness mean in plain English? 

>**Answer:** If we were to repeat this experiment over and over again, sampling from the same population, 95% of the time the slope we calculate would be between 0.25 and 0.73 (i.e. in 19 out of every 20 experiments we'd get a slope in this interval).

## Confidence bands

* In addition to estimating precision around the our coefficients, we can also estimate our precision around each predicted value, $\hat{Y_i}$. These standard errors are generated by `broom::augment()` (and are labeled `.se.fit`).

```{r}
model %>% # start with our model object
  augment() %>% # from broom package; gives us fitted values, residuals, etc.
  select(sr_health, .fitted, .se.fit) # select relevant variables
```
<br>

* If we were to string all of this information together, it would generate a confidence **band** around our regression line. As we've seen already with previous examples, it's really easy to get this confidence band when creating a scatter plot by adding `geom_smooth(method = "lm")`. 


```{r}
health %>%
  ggplot(aes(x = consc, y = sr_health)) + 
  geom_point(alpha = 0.3) + 
  geom_smooth(method = "lm") + # adds a layer that includes the regression line & 95% confidence band
  labs(x = "Conscientiousness", y = "Self-rated health") +
  theme_minimal()
```


* The animation below is an example of a ["Hypothetical Outcomes Plot"](https://github.com/wilkelab/ungeviz){target="_blank"} (HOP) that demonstrates what this 95% CI band really means. If we were to repeat this same experiment over and over and estimated our regression line each time, 95% of the time our regression line would fall within this confidence band.

```{r echo=FALSE}
library(ungeviz)
library(gganimate)
library(transformr)
library(gifski)

set.seed(012220)

boots <- bootstrapper(50)

p <- health %>%
  ggplot(aes(x = consc, y = sr_health)) +
  geom_smooth(method = "lm", color = NA) +
  geom_point(alpha = 0.3) +
  geom_point(data = boots, aes(group = .row)) +
  geom_smooth(data = boots, method = "lm", fullrange = TRUE, se = FALSE) +
  theme_minimal() +
  labs(x = "Conscientiousness", y = "Self-rated health") +
  transition_states(.draw, 1, 1) +
  enter_fade() +
  exit_fade() +
  ease_aes()

animate(p, fps = 3)

```
<br>

## Prediction bands

* A regression line, by definition, corresponds to the line that gives the *mean* value of Y corresponding to each possible value of X, i.e. E(Y|X). 

* In addition, we can also predict an individual's score ($Y_i$) given any value of x. From our regression model, we have the following equation that represents how our model characterizes the relationship between conscientiousness and self-rated health: 

$$\hat{Y}_{i} = 1.6569733 + 0.4904059X_{i}$$

* For example, if we know someone's conscientiousness score is `3.5`, we can easily predict their score for self-rated health according to our model: 

$$\hat{Y}_{i} = 1.6569733 + 0.4904059*3.5 = 3.374$$

* The `predict.lm()` function gives us an easy way to get the predicted `Y` values from all the `X` values in our dataset.  

```{r}
predict.lm(model)
```

* This should look familiar, as we already have gotten this information from `broom::augment()`. 

```{r}
augment(model)$.fitted
```

* We can use this information to create prediction bands

```{r}
temp_var <- predict(model, interval="prediction")
new_df <- cbind(health, temp_var)

ggplot(new_df, aes(x = consc, y = sr_health)) +
  geom_point() +
  geom_smooth(method=lm,se=TRUE) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), fill = "blue", alpha = 0.1) + # prediction band
  labs(x = "Conscientiousness", y = "Self-rated health") +
  theme_minimal()
```

* But remember, we can plug *any* `X` value (i.e. conscientiousness score) into our equation and predict the corresponding `Y` value( i.e. self-rated health score). And when I say *any* `X` value, I mean that it doesn't have to be an actual observation from our sample. We can also predict out-of-sample! 

```{r}
# First, we'll generate a data frame that has new X values (conscientiousness scores)
# We'll create 20 new conscientiousness scores by drawing from a random normal distribution with the same mean and standard deviation as the conscientiousness variable from our original sample

consc_data_new <- data.frame(consc = rnorm(n = 20,
                                           mean = mean(health$consc), 
                                           sd = sd(health$consc)))

print(consc_data_new)
```
<br>

* Next we'll predict `Y` values for these out-of-sample `X` values. 

```{r}
predict.lm(model, newdata = consc_data_new)
```

## Other visualization tools

* `sjPlot::plot_model()`

```{r}

```


***

# Regression with matrix algebra

***

# The General Linear Model

***



***

# Minihacks

## Minihack 1: Calculating confidence intervals{#minihack1}

1. Calculating confidence intervals "by hand". 

```{r}
model_summary <- summary(model)

# extract coefficients
int <- model_summary$coefficients[[1,1]]
slope <- model_summary$coefficients[[2,1]]

# extract standard errors of coefficients
int_se <- model_summary$coefficients[[1,2]]
slope_se <- model_summary$coefficients[[2,2]]

# df for t-distribution (will be denominator df from F statistic)
df <- model_summary$fstatistic[["dendf"]]

# intercept 95% CI 
int_ci_l <- int - qt(.975, df = df)*int_se
int_ci_u <- int + qt(.975, df = df)*int_se

# slope 95% CI 
slope_ci_l <- slope - qt(.975, df = df)*slope_se
slope_ci_u <- slope + qt(.975, df = df)*slope_se
```

2. Verify that your answer corresponds to the result from `confint()`. 

```{r}
confint(model)
```


***

## Minihack 2

***

## Minihack 3

***
