<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Univariate regression II</title>
    <meta charset="utf-8" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/rladies.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/rladies-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="my-theme.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Univariate regression II

---


## Last time...

- Introduction to univariate regression

- Calculation and interpretation of `\(b_0\)` and `\(b_1\)`

- Relationship between `\(X\)`, `\(Y\)`, `\(\hat{Y}\)`, and `\(e\)` 

---

### Today...

Statistical inferences with regression

- Partitioning the variance

- Testing `\(b_{xy}\)`

---

## Statistical Inference

- The way the world is = our model + error

- How good is our model? Does it "fit" the data well? 

--

To assess how well our model fits the data, we simply take all the variability in our outcome and partition it into different categories. For now, we will partition it into two categories: the variability that is predicted by (explained by) our model, and variability that is not.

---

## Partitioning variance (variability)

When it comes to partitioning variance, we often use the sum of the squared deviations for the math.

For one, minimizing the total residual squared deviations is the goal of OLS. 

For another, squared deviations are additive, making the math relatively simple. The total squared deviations of Y is equal to the total residual squared deviations plus the variance of `\(\hat{Y}\)`:

`$$\Large \sum (Y - \bar{Y})^2 = \sum (\hat{Y} -\bar{Y})^2 + \sum(Y - \hat{Y})^2$$`

---

`$$\Large \sum (Y - \bar{Y})^2 = \sum (\hat{Y} -\bar{Y})^2 + \sum(Y - \hat{Y})^2$$`

For simplicity and readability, we may abbreviate these:

`$$\Large SS_Y = SS_{\text{Model}} + SS_{\text{Residual}}$$`

The relative magnitude of sums of squares, especially in more complex designs, provides a way of identifying particularly large and important sources of variability. In the future, we can further partition `\(SS_{\text{Model}}\)` and `\(SS_{\text{Residual}}\)` into smaller pieces, which will help us make more specific inferences and increase statistical power, respectively. 

`$$\Large s^2_Y = s^2_{\hat{Y}} + s^2_{e}$$`
---

## Partitioning variance in Y
- Consider the case with no correlation between X and Y

`$$\Large \hat{Y} = \bar{Y} + r_{xy} \frac{s_{y}}{s_{x}}(X-\bar{X})$$`
`$$\Large \hat{Y} = \bar{Y}$$`

- To the extent that we can generate different predicted values of Y based on the values of the predictors, we are doing well in our prediction

`$$\Large \sum (Y - \bar{Y})^2 = \sum (\hat{Y} -\bar{Y})^2 + \sum(Y - \hat{Y})^2$$`

`$$\Large SS_Y = SS_{\text{Model}} + SS_{\text{Residual}}$$`

---
## Coefficient of Determination

`$$\Large \frac{s_{Model}^2}{s_{y}^2} = \frac{SS_{Model}}{SS_{Y}} = R^2$$`


---
## Example

```r
fit.1 &lt;- lm(child ~ parent, data = galton.data)
summary(fit.1)
```

```
## 
## Call:
## lm(formula = child ~ parent, data = galton.data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -7.8050 -1.3661  0.0487  1.6339  5.9264 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 23.94153    2.81088   8.517   &lt;2e-16 ***
## parent       0.64629    0.04114  15.711   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.239 on 926 degrees of freedom
## Multiple R-squared:  0.2105,	Adjusted R-squared:  0.2096 
## F-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16
```

```r
summary(fit.1)$r.squared
```

```
## [1] 0.2104629
```

---
## Example


```r
cor.test(galton.data$parent, galton.data$child)
```

```
## 
## 	Pearson's product-moment correlation
## 
## data:  galton.data$parent and galton.data$child
## t = 15.711, df = 926, p-value &lt; 2.2e-16
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.4064067 0.5081153
## sample estimates:
##       cor 
## 0.4587624
```

--


```r
cor(galton.data$parent, galton.data$child)^2
```

```
## [1] 0.2104629
```

---
## Computing Sum of Squares

`$$\Large \frac{SS_{Model}}{SS_{Y}} = R^2$$`
`$$\Large SS_{Model} = R^2({SS_{Y})}$$`
`$$\Large SS_{residual} = SS_{Y} - R^2({SS_{Y})}$$`

`$$\Large  SS_{residual} = (1- R^2){SS_{Y}}$$`
???
 



![](4-regression_files/figure-html/unnamed-chunk-5-1.png)&lt;!-- --&gt;

---
## residual standard error


```
## 
## Call:
## lm(formula = child ~ parent, data = galton.data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -7.8050 -1.3661  0.0487  1.6339  5.9264 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 23.94153    2.81088   8.517   &lt;2e-16 ***
## parent       0.64629    0.04114  15.711   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.239 on 926 degrees of freedom
## Multiple R-squared:  0.2105,	Adjusted R-squared:  0.2096 
## F-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16
```

---
## residual standard error/deviation 

- aka standard deviation of the residual
- aka standard error of the estimate

`$$\Large \hat{\sigma_e}$$`

- interpreted in original units (unlike `\(R^2\)`)
- standard deviation of Y not accounted by model

---

## residual standard error/deviation  



```r
summary(fit.1)$sigma 
```

```
## [1] 2.238547
```

```r
galton.data.1 = broom::augment(fit.1)
psych::describe(galton.data.1$.resid)
```

```
##    vars   n mean   sd median trimmed  mad   min  max range  skew kurtosis   se
## X1    1 928    0 2.24   0.05    0.06 2.26 -7.81 5.93 13.73 -0.24    -0.23 0.07
```

---
## residual standard error/deviation 


```r
summary(fit.1)$sigma 
```

```
## [1] 2.238547
```

```r
psych::describe(galton.data$parent)
```

```
##    vars   n  mean   sd median trimmed  mad min max range  skew kurtosis   se
## X1    1 928 68.31 1.79   68.5   68.32 1.48  64  73     9 -0.04     0.05 0.06
```
---


![](4-regression_files/figure-html/unnamed-chunk-9-1.png)&lt;!-- --&gt;
---
## r2 and residual standard deviation
- two sides of same coin
- one in original units, the other standardized 
- R2 can be tricky because the numerator and denominator can be changed in different ways. 
- for example if variance in Y is changed but with the same model and residual standard error R2 could decline or increase

---
## standard errors for b
- represent our uncertainty (noise) in our estimate of the regression coefficient 
- different from residual standard error/deviation (but proportional to)  
- much like previously we can take our estimate (b) and put confidence regions around it to get an estimate of what could be "possible" if we ran the study again  
- (see equation later)
---
## data generating process

$$\Large Y_{i} \sim \mathcal{N}(\mu,\,\sigma)\ $$
Our DV for individual i is distributed normally with a mean of mu 
and a standard deviation of sigma  

--

- this describes how we think our DVs are generated, and the paramters of interest  

--

- a standard regression model assumes this, but we will see other DGPs such as binomial or poisson that do not  
- a different DGP will define what "type" of regression to use  
- for normal, `\(\mu\)` gets all the focus but `\(\sigma\)` is just as important 


---
## Inferential tests
### Omnibus test

`$$\Large H_{0}: \rho_{XY}^2= 0$$`
`$$\Large H_{1}: \rho_{XY}^2 \neq 0$$`

`$$\Large F = \frac{MS_{Model}}{MS_{residial}}$$`
---
## model comparison

- Last semester you looked at different models to see how their F and SSs changed depending on what predictors were included. We can do the same thing with regression models!   

- The basic idea is asking how much variance remains unexplained in our model. This "left over" variance can be contrasted with an alternative model/hypothesis. We can ask does adding a new predictor variable help explain more variance or should we stick wtih a parsimonious model.   

- Every model test you do implicitly implies you favoring that over an alternative model, typically the null. This framework allows you to be more flexible and explicit.   

---

```r
fit.1 &lt;- lm(parent ~ child, data = galton.data)
fit.0 &lt;- lm(parent ~ 1, data = galton.data)
```


---

```r
summary(fit.0)
```

```
## 
## Call:
## lm(formula = parent ~ 1, data = galton.data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.3082 -0.8082  0.1918  1.1918  4.6918 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 68.30819    0.05867    1164   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.787 on 927 degrees of freedom
```

---

```r
summary(fit.1)
```

```
## 
## Call:
## lm(formula = parent ~ child, data = galton.data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.6702 -1.1702 -0.1471  1.1324  4.2722 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 46.13535    1.41225   32.67   &lt;2e-16 ***
## child        0.32565    0.02073   15.71   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.589 on 926 degrees of freedom
## Multiple R-squared:  0.2105,	Adjusted R-squared:  0.2096 
## F-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16
```


---

```r
anova(fit.0)
```

```
## Analysis of Variance Table
## 
## Response: parent
##            Df Sum Sq Mean Sq F value Pr(&gt;F)
## Residuals 927 2961.4  3.1946
```

---

```r
anova(fit.1)
```

```
## Analysis of Variance Table
## 
## Response: parent
##            Df  Sum Sq Mean Sq F value    Pr(&gt;F)    
## child       1  623.26  623.26  246.84 &lt; 2.2e-16 ***
## Residuals 926 2338.10    2.52                      
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```


---

```r
anova(fit.1, fit.0)
```

```
## Analysis of Variance Table
## 
## Model 1: parent ~ child
## Model 2: parent ~ 1
##   Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    
## 1    926 2338.1                                  
## 2    927 2961.4 -1   -623.26 246.84 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```
---
## model comparisons 

- Model comparisons are redundent with nil/null hypotheses and coefficient tests right now, but will be more flexible down the road. 
- Key is to start thikning about your implicit alternative models
- The ultimate goal would be to create two models that represent two equally plausible theories. 
- Theory A is made up of components XYZ, whereas theory B has QRS components. You can then ask which theory(model) is better? 
---
## regression coefficient

`$$\Large H_{0}: \beta_{1}= 0$$`
`$$\Large H_{1}: \beta_{1} \neq 0$$`
---
## What does the regression coefficient test?
- Does X provide any predictive information? 
- Does X provide any explanatory power regarding the variability of Y? 
- Is the the average value the best guess (i.e., is Y bar equal to the predicted value of Y?)
- Is the regression line flat? 
- Are X and Y correlated?  
---
## Regression coefficient
`$$\Large se_{b} = \frac{s_{Y}}{s_{X}}{\sqrt{\frac {1-r_{xy}^2}{n-2}}}$$`
`$$\Large t(n-2) = \frac{b_{1}}{se_{b}}$$`
** what is standardized equation? 
---
## Intercept

- same idea, more complex se calculation as the calculation depends on how far the X value (here zero) is away from the mean of X
- farther from the mean, less information, thus more uncertainty 
- we will come back and see this equation later
---
## Confidence interval for coefficents

- same equation as we've been working with
- estimate plus minus 1.96*se
---
## Confidence Bands for regression line

```
## Warning: Removed 19 rows containing non-finite values (stat_smooth).
```

```
## Warning: Removed 19 rows containing missing values (geom_point).
```

```
## Warning: Removed 1 rows containing non-finite values (stat_smooth).
```

```
## Warning: Removed 1 rows containing missing values (geom_point).
```

![](4-regression_files/figure-html/unnamed-chunk-16-1.png)&lt;!-- --&gt;
---
## Confidence Bands
- Compare mean estimate for height of 70 based on regression vs binning
- Model uses all data where binning uses much less

![](4-regression_files/figure-html/unnamed-chunk-17-1.png)&lt;!-- --&gt;
---
## Confidence Bands

$$\Large \hat{Y}\pm t_{critical} * se_{residual}*\sqrt{\frac {1}{n}+\frac{(X-\bar{X})^2}{(n-1)s_{X}^2}} $$


---
## Prediction
- very similar to confidence bands around regressions line
- differences is that we are predicting and individual i's score, not the Y hat for a particular level of X. (A new Yi given x, rather than Ybar given x)
- Because there is greater variation in predicting an individual value rather than a collection of individual values (ie the mean) the prediction band is greater
- Combines unknown variability in 1) the estimated mean (as reflected in se of b) 2) peoples scores around  mean (residual standard error) 

$$\Large \hat{Y}\pm t_{critical} * se_{residual}*\sqrt{1+ \frac {1}{n}+\frac{(X-\bar{X})^2}{(n-1)s_{X}^2}} $$

---


```r
temp_var &lt;- predict(fit.1, interval="prediction")
new_df &lt;- cbind(galton.data, temp_var)
pred &lt;- ggplot(new_df, aes(x=child, y=parent))+
       geom_point() +   
  geom_smooth(method=lm,se=TRUE) +
 geom_ribbon(aes(ymin = lwr, ymax = upr), 
               fill = "blue", alpha = 0.1)
```


---

```r
pred
```

![](4-regression_files/figure-html/unnamed-chunk-19-1.png)&lt;!-- --&gt;

---
## Mean square error (MSE)


- AKA means square residual/within

- unbiased estimate of error variance

- measure of discrepancy between the data and the model

- the MSE is the variance around the fitted regression line

- just like `\(\text{MS}_{\text{within}}\)` was variance around predicted group means
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
