---
title: 'Psy 612: Data Analysis II'
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [default, rladies, rladies-fonts, "my-theme.css"]
    incremental: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

## Welcome back!

**Last term:**

- Probability, sampling, hypothesis testing
- Descriptive statistics
- A little matrix algebra

**This term:**

- Model building

--
- Correlations
- Linear regression
- General linear model
- Multiple regresesion

---

## Website

[UOpsych.github.io/psy612/](uopsych.github.io/psy612/)

Structure of this course:
 - Lectures, Labs, Reading 
 - Weekly quizzes (T/F)
 - Homework assignments (2)
 - Final project (1)
 
If you took PSY 611 in Fall 2018 or earlier, take a look at the materials for last term:
[UOpsych.github.io/psy611/](uopsych.github.io/psy611/)


---
## Relationships

- What is the relationship between IV and DV?

- Measuring relationships depend on type of measurement

- You have primarily been working wtih categorical IVs (*t*-test, chi-square)

---
## Scatter Plot with best fit line
```{r,echo=FALSE, message=FALSE, cache=TRUE}
library(psych)
library(tidyverse)
library(RColorBrewer)
galton.data <- galton
purples = brewer.pal(n = 3, name = "Purples")
```

```{r, echo=FALSE, cache=TRUE}
ggplot(galton.data, aes(x=parent, y=child)) +
    geom_jitter() +    
    geom_smooth(method=lm,   # Add linear regression line
                se=FALSE) +     # Don't add shaded confidence region
    labs(x = "parent height", y = "child height")
```

---
## Review of Dispersion

Variation (sum of squares)

$$SS = {\sum{(x-\bar{x})^2}}$$
$$SS = {\sum{(x-\mu)^2}}$$
---

## Review of Dispersion
Variance

$$\large s^{2} = {\frac{\sum{(x-\bar{x})^2}}{N-1}}$$

$$\large\sigma^{2} = {\frac{\sum{(x-\mu)^2}}{N}}$$

---

## Review of Dispersion

Standard Deviation

$$\large s = \sqrt{\frac{\sum{(x-\bar{x})^2}}{N-1}}$$

$$\large\sigma = \sqrt{\frac{\sum{(x-\mu)^2}}{N}}$$

---

class: center

Formula for standard error of the mean?

--

$$\sigma_M = \frac{\sigma}{\sqrt{N}}$$

$$\sigma_M = \frac{\hat{s}}{\sqrt{N}}$$

---
## Associations

- i.e., relationships
- to look at continuous variable associations we need to think in terms of how variables relate to one another

---
## Associations

Covariation (cross products)

**Sample:**

$$\large SS = {\sum{(x-\bar{x})(y-\bar{y})}}$$

**Population:**

$$SS = {\sum{{(x-\mu_{x}})(y-\mu_{y})}}$$

---
## Associations

Covariance

**Sample:**

$$\large cov_{xy}^{2} = {\frac{\sum{(x-\bar{x})(y-\bar{y})}}{N-1}}$$

**Population:**

$$\large \sigma_{xy}^{2} = {\frac{\sum{(x-\mu_{x})(y-\mu_{y})}}{N}}$$

>- Covariance matrix is basis for many analyses
>- What are some issues that may arise when comparing covariances?

---
## Associations
Correlations

**Sample:**

$$\large r_{xy} = {\frac{\sum({z_{x}z_{y})}}{N}}$$

**Population:**

$$\large \rho_{xy} = {\frac{cov(X,Y)}{\sigma_{x}\sigma_{y}}}$$


Many other formulas exist for specific types of data, these were more helpful when we computed everything by hand (more on this later).


---

## Correlations


- How much two variables are linearly related
- -1 to 1
- Invariant to changes in mean or scaling
- Most common (and basic) effect size measure
- Will use to build our regression model

---

## Correlations

```{r, echo=FALSE, cache=TRUE}
ggplot(galton.data, aes(x=parent, y=child)) +
    geom_jitter(alpha = .4) +
    geom_smooth(method=lm,   # Add linear regression line
                se=FALSE) +     # Don't add shaded confidence region
    labs(x = "parent height", y = "child height") +
  theme_bw()
```


---
## Statistical test
Hypothesis testing

$$\large H_{0}: \rho_{xy} = 0$$

$$\large H_{A}: \rho_{xy} \neq 0$$

Assumes:
- Observations are independent
- Symmetric bivariate distribution (joint probability distribution)
    - Not necessarily normal

---

Recall the assumption of normality for a *t*-test:

```{r, echo = F, message = F, warning = F}
library(tidyverse)
ggplot(data.frame(x = seq(-10, 10)), aes(x)) +
  stat_function(fun = function(x) dnorm(x, m = 0, sd = 10/sqrt(10)),
                geom = "area", fill = purples[3]) +
  scale_x_continuous("X", labels = NULL, limits = c(-10,10)) +
  scale_y_continuous("density", labels = NULL) +
  theme_bw()
```



---

```{r, echo=FALSE,cache=TRUE}
mu1<-0 # setting the expected value of x1
mu2<-0 # setting the expected value of x2
s11<-10 # setting the variance of x1
s12<-15 # setting the covariance between x1 and x2
s22<-10 # setting the variance of x2
rho<-0.5 # setting the correlation coefficient between x1 and x2
x1<-seq(-10,10,length=41) # generating the vector series x1
x2<-x1 # copying x1 to x2
f<-function(x1,x2)
{
term1<-1/(2*pi*sqrt(s11*s22*(1-rho^2)))
term2<--1/(2*(1-rho^2))
term3<-(x1-mu1)^2/s11
term4<-(x2-mu2)^2/s22
term5<--2*rho*((x1-mu1)*(x2-mu2))/(sqrt(s11)*sqrt(s22))
term1*exp(term2*(term3+term4-term5))
} # setting up the function of the multivariate normal density
#
z<-outer(x1,x2,f) # calculating the density values
#
persp(x1, x2, z,
main="Joint Probability Distribution", sub=expression(italic(f)~(bold(x))==frac(1,2~pi~sqrt(sigma[11]~ sigma[22]~(1-rho^2)))~phantom(0)^bold(.)~exp~bgroup("{", list(-frac(1,2(1-rho^2)),
bgroup("[", frac((x[1]~-~mu[1])^2, sigma[11])~-~2~rho~frac(x[1]~-~mu[1], sqrt(sigma[11]))~ frac(x[2]~-~mu[2],sqrt(sigma[22]))~+~ frac((x[2]~-~mu[2])^2, sigma[22]),"]")),"}")),
col= purples[3],
theta=30, phi=20,
r=50,
d=0.1,
expand=0.5,
ltheta=90, lphi=180,
shade=0.75,
ticktype="detailed",
nticks=5)

# produces the 3-D plot
mtext(expression(list(mu[1]==0,mu[2]==0,sigma[11]==10,sigma[22]==10,sigma[12 ]==15,rho==0.5)), side=3)
# adding a text line to the graph


```

---
## Statistical test


$$\large H_{0}: \rho_{xy} = 0$$

$$\large H_{A}: \rho_{xy} \neq 0$$


Test statistic

$$\large t = {\frac{r}{SE_{r}}}$$
--
.pull-left[
$$\large SE_r = \sqrt{\frac{1-r^2}{N-2}}$$

$$\large t = {\frac{r}{\sqrt{\frac{1-r^{2}}{N-2}}}}$$
]


--

.pull-right[
$$\large DF = N-2$$
]

---
## Effect size

- The strength of relationship between two variables

- $\eta^2$, cohen’s d, cohen’s f, hedges g, $R^2$ , Risk-ratio, etc

- Significance is a function of effect size and sample size

- Statistical significance $\neq$ practical significance

---
## Effect size
How big is practical?

- Cohen (.1, .3., .5)
- Meyer & Hemphill .3 is average
- Rosenthaul:

Drug TX?  | Alive |  Dead
----------|-------|--------
Treatment |  65   |  35
No Tx     |  35   |  65


---
## What is the size of the correlation?
- Chemotherapy and breast cancer survival?
- Batting ability and hit success on a single at bat?
- Antihistamine use and reduced sneezing/runny nose?
- Combat exposure and PTSD?
- Ibuprofen on pain reduction?
- Gender and weight?
- Therapy and well being?
- Observer ratings of attractiveness?
- Gender and arm strength?
- Nearness to equator and daily temperature for U.S.?

---
## What is the size of the correlation?
- Chemotherapy and breast cancer survival? (.03)
- Batting ability and hit success on a single at bat? (.06)
- Antihistamine use and reduced sneezing/runny nose? (.11)
- Combat exposure and PTSD? (.11)
- Ibuprofen on pain reduction? (.14)
- Gender and weight? (.26)
- Therapy and well being? (.32)
- Observer ratings of attractiveness? (.39)
- Gender and arm strength? (.55)
- Nearness to equator and daily temperature for U.S.? (.60)

---
## Questions to ask yourself:
- What is your N?
- What is the typical effect size in the field?
- Study design?
- What is your DV?
- Importance?
- Same method as IV (method variance)?

---
## Power calculations
```{r}
library(pwr)
pwr.r.test(n = , r = .1, sig.level = .05 , power = .8)
pwr.r.test(n = , r = .3, sig.level = .05 , power = .8)
```

---
## Power calculations
- But what is your confidence?
- N = 84 gives you CI[.09, .48]
- Schönbrodt & Perugini (2013) suggest correlations 'stabilize' at 250+ regardless of effect size
- CI[.19, .39]

---
## Fisher’s r to z’ transformation
.left-column[If we want to make calculations based on $\rho \neq 0$ then we will run into a skewed sampling distribution]

```{r,echo=FALSE, cache=TRUE }
library(ggplot2)

r=0.95
n=50
rep=10000
z=.5*log((1+r)/(1-r))
se=1/sqrt(n-3)
zvec=rnorm(rep)*se+z
ivec=(exp(2*zvec)-1)/(exp(2*zvec)+1)
correlation <- as.data.frame(ivec)
correlation$cor <- correlation$ivec
ggplot(correlation, aes(cor)) + geom_histogram(breaks=seq(.85, .99, by = .001), aes(y = ..density..)) + geom_density()
```

---
## Fisher’s r to z’ transformation
- Skewed sampling distribution will rear its head when:
    * $H_{0}: \rho \neq 0$
    * Calculating confidence intervals
    * Testing two correlations against one another

---

```{r, echo = F}
library(psych)
data.frame(r_val = c(-.3, 0, .9)) %>%
  mutate(N = 25,
         z_se = 1/sqrt(N-3),
         z_mean = fisherz(r_val)) %>%
  mutate(density = map2(z_mean, z_se,
                        function(x,y) dnorm(x = seq(-2,2, by = .1),
                                            mean = x, sd = y))) %>%
  unnest(cols = c(density)) %>%
  mutate(r_density = rep(fisherz2r(seq(-2,2, by = .1)), 3)) %>%
  ggplot(aes(x = r_density, y = density, color = r_val)) +
  facet_wrap(~r_val)


```


---
## Fisher’s r to z’ transformation

- r to z':

$$\largez^{'} = {\frac{1}{2}}ln{\frac{1+r}{1-r}}$$

---
## Fisher’s r to z’ transformation

```{r}
r = seq(-.99,.99,.01)
z = psych::fisherz(r)
data.frame(r,z) %>%
  ggplot(aes(x = r, y = z)) +
  geom_line() +
  scale_x_continuous(expr(r))+
  scale_y_continuous(expr(z_r))+
  theme_bw()
```


---
##  Steps for computing confidence interval

1. Transform r into z'
2. Compute CI as you normally would using z'
3. revert back to r

$$\larger = {\frac{e^{2z'}-1}{e^{2z'}+1}}$$

---
## Other correlation tests:
1. Set of correlations
2. Dependent correlations (i.e., within same group)
  These are more easily tested via Structural Equation Modeling (SEM)
3. Intra Class Correlation (ICC)

- Again, best to do these tests in another framework (e.g., interaction, SEM, MLM)

---
## Factors that influence r (and most other test statistics)
1. Restriction of range (GRE scores and success)
2. Very skewed distributions (smoking and health)
3. Non-linear associations
4. Measurement overlap (modality and content)
5. Reliability

---
## Reliability
- All measurement includes error
- Score = true score + measurement error (CTT version)
- Reliability assesses the consistency of measurement

--
  Which would you rather have?
  - 1-item final exam versus 30-item?
  - assessment via trained clinician vs tarot cards?
  - fMRI during minor earthquake vs no earthquake?

---
## Reliability

- Cannot correlate error (randomness) with something
- Because we do not measure our variables perfectly we get lower correlations compared to true correlations
- If we want to have a valid measure it better be a reliable measure

---
## Reliability

- think of reliability as a correlation with a measure and itself in a different world, at a different time, or a different but equal version

$$\larger_{XX}$$

---
## Reliability

- true score variance divided by observed variance
- how do you assess theoretical variance i.e., true score variance?

$$\larger_{XY} = r_{X_{T} Y_{T}} {\sqrt{r_{XX}r_{YY}}}$$
$$\larger_{XY} = .6 {\sqrt {(.70) (.70)}}$$

---
## Reliability

$$\larger_{X_{T} Y_{T}} =  = {\frac {r_{XY}} {\sqrt{r_{XX}r_{YY}}}}$$


$$\larger_{X_{T} Y_{T}} =  = {\frac {.30} {\sqrt{(.70)(.70)}}}$$

---
## Most common ways to assess

- cronbachs alpha
```{r, eval=FALSE}
library (psych)
alpha(measure)
## Gives average split half correlation
## Can tell you if you are assessing a single construct
```
- test - retest reliability
- Kappa or ICC

---
## Reliability

- if you are going to measure something do it well
- applies to ALL IVs and DVs, and all designs
- remember this when interpretting other's research

---
## Types of correlations
- Many ways to get at relationship between two variables
- Statistically the different types are almost exactly the same
- Exist for historical reasons

---
## Types of correlations
1. Point Biserial
    +  continuous and dichtomous
2. Phi coefficient
    + both dichotomous
3. Spearman rank order
    + ranked data (nonparametric)
4. Biserial (assumes dichotomous is continous)
5. Tetrachoric (assumes dichotomous is continous)
    + both dichotomous
6. Polychoric (assumes continous)
    + ordinal


